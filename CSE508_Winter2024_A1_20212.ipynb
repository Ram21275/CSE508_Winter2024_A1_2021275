{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBybQfjmVYfo",
        "outputId": "dc4033ea-e7b4-4095-88dc-479c4b82fb32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.10/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "!pip install contractions\n",
        "import contractions\n",
        "from collections import defaultdict\n",
        "from tqdm.notebook import tqdm as tqdm\n",
        "import ast\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "loc = \"/content/drive/MyDrive/text_files/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOpDP33PGwl9",
        "outputId": "fc2b411f-ee28-49b4-dff5-6a2c4d0e29ef"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def bold_text(text):\n",
        "  return \"\\033[4m\" + text + \"\\033[0m\""
      ],
      "metadata": {
        "id": "XOeFL1H-L8Ea"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_file(file_path, i):\n",
        "    with open(file_path, 'r') as file:\n",
        "        # Read the file\n",
        "        text = file.read()\n",
        "        t1 = text\n",
        "\n",
        "        # Lowercase the text\n",
        "        text = text.lower()\n",
        "        t2 = text\n",
        "\n",
        "        # Remove HTML tags using BeautifulSoup\n",
        "        soup = BeautifulSoup(text, \"html.parser\")\n",
        "        text = soup.get_text()\n",
        "        t3 = text\n",
        "\n",
        "        # Perform tokenization\n",
        "        tokens = word_tokenize(text)\n",
        "        t4 = tokens\n",
        "\n",
        "        # Remove stopwords and punctuations\n",
        "        tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]\n",
        "        t5 = tokens\n",
        "\n",
        "        # Remove blank space tokens\n",
        "        tokens = [word for word in tokens if word.strip()]\n",
        "        t6 = tokens\n",
        "\n",
        "        if(i>=500 and i<505):\n",
        "          print()\n",
        "          print(bold_text(\"Before lowercasing:\")+ f\" {t1}\")\n",
        "          print()\n",
        "          print(bold_text(\"After lowercasing:\")+ f\" {t2}\")\n",
        "          print()\n",
        "          print(bold_text(\"Before tokenization:\")+ f\" {t3}\")\n",
        "          print()\n",
        "          print(bold_text(\"After tokenization:\")+ f\" {t4}\")\n",
        "          print()\n",
        "          print(bold_text(\"Before removing stopwords and punctuations:\") + f\" {t4}\")\n",
        "          print()\n",
        "          print(bold_text(\"After removing stopwords and punctuations:\") + f\" {t5}\")\n",
        "          print()\n",
        "          print(bold_text(\"Before removing blank space:\") + f\" {t5}\")\n",
        "          print()\n",
        "          print(bold_text(\"After removing blank space:\") + f\" {t6}\")\n",
        "          print()\n",
        "          print(\"************************************************\")\n",
        "\n",
        "        return tokens"
      ],
      "metadata": {
        "id": "gE68ggrRHhyQ"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pp_files(file_path, content):\n",
        "  with open(file_path, 'w') as f:\n",
        "    f.write(content)"
      ],
      "metadata": {
        "id": "yuHaWDdvlaZT"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process files named file1.txt through file999.txt\n",
        "loc2 = \"/content/drive/MyDrive/IR_Dataset/\"\n",
        "stop_words = set(stopwords.words('english'))\n",
        "preprossed_files = []\n",
        "for i in range(1, 1000):\n",
        "    file_name = loc+f\"file{i}.txt\"\n",
        "    tokens = preprocess_file(file_name, i)\n",
        "    tokens = \" \".join(tokens)\n",
        "      # print(tokens)\n",
        "\n",
        "    preprossed_fileName = loc2+f\"pp_file{i}.txt\"\n",
        "    pp_files(preprossed_fileName, tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uiexf4zMH5b1",
        "outputId": "19e623b0-0132-47ea-c21f-b56bb8c9564d"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-128-9cac72885ed1>:12: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"html.parser\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[4mBefore lowercasing:\u001b[0m Outstanding case!  My H6 fits into this case perfectly.  It is made of heavy duty plastic, which is at least twice as thick at the case that comes with the H6.  Each cut-out within the foam interior is custom sized to fit your H6 and all of the accessories.  The latches are very sturdy.  See picture to get a sense of the size in relation to the standard H6 case.  BUY IT...\n",
            "\n",
            "\u001b[4mAfter lowercasing:\u001b[0m outstanding case!  my h6 fits into this case perfectly.  it is made of heavy duty plastic, which is at least twice as thick at the case that comes with the h6.  each cut-out within the foam interior is custom sized to fit your h6 and all of the accessories.  the latches are very sturdy.  see picture to get a sense of the size in relation to the standard h6 case.  buy it...\n",
            "\n",
            "\u001b[4mBefore tokenization:\u001b[0m outstanding case!  my h6 fits into this case perfectly.  it is made of heavy duty plastic, which is at least twice as thick at the case that comes with the h6.  each cut-out within the foam interior is custom sized to fit your h6 and all of the accessories.  the latches are very sturdy.  see picture to get a sense of the size in relation to the standard h6 case.  buy it...\n",
            "\n",
            "\u001b[4mAfter tokenization:\u001b[0m ['outstanding', 'case', '!', 'my', 'h6', 'fits', 'into', 'this', 'case', 'perfectly', '.', 'it', 'is', 'made', 'of', 'heavy', 'duty', 'plastic', ',', 'which', 'is', 'at', 'least', 'twice', 'as', 'thick', 'at', 'the', 'case', 'that', 'comes', 'with', 'the', 'h6', '.', 'each', 'cut-out', 'within', 'the', 'foam', 'interior', 'is', 'custom', 'sized', 'to', 'fit', 'your', 'h6', 'and', 'all', 'of', 'the', 'accessories', '.', 'the', 'latches', 'are', 'very', 'sturdy', '.', 'see', 'picture', 'to', 'get', 'a', 'sense', 'of', 'the', 'size', 'in', 'relation', 'to', 'the', 'standard', 'h6', 'case', '.', 'buy', 'it', '...']\n",
            "\n",
            "\u001b[4mBefore removing stopwords and punctuations:\u001b[0m ['outstanding', 'case', '!', 'my', 'h6', 'fits', 'into', 'this', 'case', 'perfectly', '.', 'it', 'is', 'made', 'of', 'heavy', 'duty', 'plastic', ',', 'which', 'is', 'at', 'least', 'twice', 'as', 'thick', 'at', 'the', 'case', 'that', 'comes', 'with', 'the', 'h6', '.', 'each', 'cut-out', 'within', 'the', 'foam', 'interior', 'is', 'custom', 'sized', 'to', 'fit', 'your', 'h6', 'and', 'all', 'of', 'the', 'accessories', '.', 'the', 'latches', 'are', 'very', 'sturdy', '.', 'see', 'picture', 'to', 'get', 'a', 'sense', 'of', 'the', 'size', 'in', 'relation', 'to', 'the', 'standard', 'h6', 'case', '.', 'buy', 'it', '...']\n",
            "\n",
            "\u001b[4mAfter removing stopwords and punctuations:\u001b[0m ['outstanding', 'case', 'h6', 'fits', 'case', 'perfectly', 'made', 'heavy', 'duty', 'plastic', 'least', 'twice', 'thick', 'case', 'comes', 'h6', 'cut-out', 'within', 'foam', 'interior', 'custom', 'sized', 'fit', 'h6', 'accessories', 'latches', 'sturdy', 'see', 'picture', 'get', 'sense', 'size', 'relation', 'standard', 'h6', 'case', 'buy', '...']\n",
            "\n",
            "\u001b[4mBefore removing blank space:\u001b[0m ['outstanding', 'case', 'h6', 'fits', 'case', 'perfectly', 'made', 'heavy', 'duty', 'plastic', 'least', 'twice', 'thick', 'case', 'comes', 'h6', 'cut-out', 'within', 'foam', 'interior', 'custom', 'sized', 'fit', 'h6', 'accessories', 'latches', 'sturdy', 'see', 'picture', 'get', 'sense', 'size', 'relation', 'standard', 'h6', 'case', 'buy', '...']\n",
            "\n",
            "\u001b[4mAfter removing blank space:\u001b[0m ['outstanding', 'case', 'h6', 'fits', 'case', 'perfectly', 'made', 'heavy', 'duty', 'plastic', 'least', 'twice', 'thick', 'case', 'comes', 'h6', 'cut-out', 'within', 'foam', 'interior', 'custom', 'sized', 'fit', 'h6', 'accessories', 'latches', 'sturdy', 'see', 'picture', 'get', 'sense', 'size', 'relation', 'standard', 'h6', 'case', 'buy', '...']\n",
            "\n",
            "************************************************\n",
            "\n",
            "\u001b[4mBefore lowercasing:\u001b[0m The cable came just as advertised, it was thick, well made and appealing to the sight and touch. Customer care was excellent and I haven't even used the cable yet. I'm positive it will Proform as good as the company has with its follow up on purchase. This is what anyone who shops hopes for in a company and it is rare in today's consumer goods climate. That you very much and from that I highly recommend this product to all customers seeking cables for there Pro gear. DJ Sinista 1 -Alwayz Mobile Productions.\n",
            "\n",
            "\u001b[4mAfter lowercasing:\u001b[0m the cable came just as advertised, it was thick, well made and appealing to the sight and touch. customer care was excellent and i haven't even used the cable yet. i'm positive it will proform as good as the company has with its follow up on purchase. this is what anyone who shops hopes for in a company and it is rare in today's consumer goods climate. that you very much and from that i highly recommend this product to all customers seeking cables for there pro gear. dj sinista 1 -alwayz mobile productions.\n",
            "\n",
            "\u001b[4mBefore tokenization:\u001b[0m the cable came just as advertised, it was thick, well made and appealing to the sight and touch. customer care was excellent and i haven't even used the cable yet. i'm positive it will proform as good as the company has with its follow up on purchase. this is what anyone who shops hopes for in a company and it is rare in today's consumer goods climate. that you very much and from that i highly recommend this product to all customers seeking cables for there pro gear. dj sinista 1 -alwayz mobile productions.\n",
            "\n",
            "\u001b[4mAfter tokenization:\u001b[0m ['the', 'cable', 'came', 'just', 'as', 'advertised', ',', 'it', 'was', 'thick', ',', 'well', 'made', 'and', 'appealing', 'to', 'the', 'sight', 'and', 'touch', '.', 'customer', 'care', 'was', 'excellent', 'and', 'i', 'have', \"n't\", 'even', 'used', 'the', 'cable', 'yet', '.', 'i', \"'m\", 'positive', 'it', 'will', 'proform', 'as', 'good', 'as', 'the', 'company', 'has', 'with', 'its', 'follow', 'up', 'on', 'purchase', '.', 'this', 'is', 'what', 'anyone', 'who', 'shops', 'hopes', 'for', 'in', 'a', 'company', 'and', 'it', 'is', 'rare', 'in', 'today', \"'s\", 'consumer', 'goods', 'climate', '.', 'that', 'you', 'very', 'much', 'and', 'from', 'that', 'i', 'highly', 'recommend', 'this', 'product', 'to', 'all', 'customers', 'seeking', 'cables', 'for', 'there', 'pro', 'gear', '.', 'dj', 'sinista', '1', '-alwayz', 'mobile', 'productions', '.']\n",
            "\n",
            "\u001b[4mBefore removing stopwords and punctuations:\u001b[0m ['the', 'cable', 'came', 'just', 'as', 'advertised', ',', 'it', 'was', 'thick', ',', 'well', 'made', 'and', 'appealing', 'to', 'the', 'sight', 'and', 'touch', '.', 'customer', 'care', 'was', 'excellent', 'and', 'i', 'have', \"n't\", 'even', 'used', 'the', 'cable', 'yet', '.', 'i', \"'m\", 'positive', 'it', 'will', 'proform', 'as', 'good', 'as', 'the', 'company', 'has', 'with', 'its', 'follow', 'up', 'on', 'purchase', '.', 'this', 'is', 'what', 'anyone', 'who', 'shops', 'hopes', 'for', 'in', 'a', 'company', 'and', 'it', 'is', 'rare', 'in', 'today', \"'s\", 'consumer', 'goods', 'climate', '.', 'that', 'you', 'very', 'much', 'and', 'from', 'that', 'i', 'highly', 'recommend', 'this', 'product', 'to', 'all', 'customers', 'seeking', 'cables', 'for', 'there', 'pro', 'gear', '.', 'dj', 'sinista', '1', '-alwayz', 'mobile', 'productions', '.']\n",
            "\n",
            "\u001b[4mAfter removing stopwords and punctuations:\u001b[0m ['cable', 'came', 'advertised', 'thick', 'well', 'made', 'appealing', 'sight', 'touch', 'customer', 'care', 'excellent', \"n't\", 'even', 'used', 'cable', 'yet', \"'m\", 'positive', 'proform', 'good', 'company', 'follow', 'purchase', 'anyone', 'shops', 'hopes', 'company', 'rare', 'today', \"'s\", 'consumer', 'goods', 'climate', 'much', 'highly', 'recommend', 'product', 'customers', 'seeking', 'cables', 'pro', 'gear', 'dj', 'sinista', '1', '-alwayz', 'mobile', 'productions']\n",
            "\n",
            "\u001b[4mBefore removing blank space:\u001b[0m ['cable', 'came', 'advertised', 'thick', 'well', 'made', 'appealing', 'sight', 'touch', 'customer', 'care', 'excellent', \"n't\", 'even', 'used', 'cable', 'yet', \"'m\", 'positive', 'proform', 'good', 'company', 'follow', 'purchase', 'anyone', 'shops', 'hopes', 'company', 'rare', 'today', \"'s\", 'consumer', 'goods', 'climate', 'much', 'highly', 'recommend', 'product', 'customers', 'seeking', 'cables', 'pro', 'gear', 'dj', 'sinista', '1', '-alwayz', 'mobile', 'productions']\n",
            "\n",
            "\u001b[4mAfter removing blank space:\u001b[0m ['cable', 'came', 'advertised', 'thick', 'well', 'made', 'appealing', 'sight', 'touch', 'customer', 'care', 'excellent', \"n't\", 'even', 'used', 'cable', 'yet', \"'m\", 'positive', 'proform', 'good', 'company', 'follow', 'purchase', 'anyone', 'shops', 'hopes', 'company', 'rare', 'today', \"'s\", 'consumer', 'goods', 'climate', 'much', 'highly', 'recommend', 'product', 'customers', 'seeking', 'cables', 'pro', 'gear', 'dj', 'sinista', '1', '-alwayz', 'mobile', 'productions']\n",
            "\n",
            "************************************************\n",
            "\n",
            "\u001b[4mBefore lowercasing:\u001b[0m Kit is awesome. I play in my garage just for personal enjoyment not for performances or anything. Once you take the time to break down all the settings, your able to dial in pretty much any kit and sound. With the expansion options and the relatively inexpensive parts expanding is easy and fun.\n",
            "\n",
            "After a few weeks of daily use for at least an hour a day it still looks and plays beautifully. Overall one of the best purchases I could have made.\n",
            "\n",
            "\u001b[4mAfter lowercasing:\u001b[0m kit is awesome. i play in my garage just for personal enjoyment not for performances or anything. once you take the time to break down all the settings, your able to dial in pretty much any kit and sound. with the expansion options and the relatively inexpensive parts expanding is easy and fun.\n",
            "\n",
            "after a few weeks of daily use for at least an hour a day it still looks and plays beautifully. overall one of the best purchases i could have made.\n",
            "\n",
            "\u001b[4mBefore tokenization:\u001b[0m kit is awesome. i play in my garage just for personal enjoyment not for performances or anything. once you take the time to break down all the settings, your able to dial in pretty much any kit and sound. with the expansion options and the relatively inexpensive parts expanding is easy and fun.\n",
            "\n",
            "after a few weeks of daily use for at least an hour a day it still looks and plays beautifully. overall one of the best purchases i could have made.\n",
            "\n",
            "\u001b[4mAfter tokenization:\u001b[0m ['kit', 'is', 'awesome', '.', 'i', 'play', 'in', 'my', 'garage', 'just', 'for', 'personal', 'enjoyment', 'not', 'for', 'performances', 'or', 'anything', '.', 'once', 'you', 'take', 'the', 'time', 'to', 'break', 'down', 'all', 'the', 'settings', ',', 'your', 'able', 'to', 'dial', 'in', 'pretty', 'much', 'any', 'kit', 'and', 'sound', '.', 'with', 'the', 'expansion', 'options', 'and', 'the', 'relatively', 'inexpensive', 'parts', 'expanding', 'is', 'easy', 'and', 'fun', '.', 'after', 'a', 'few', 'weeks', 'of', 'daily', 'use', 'for', 'at', 'least', 'an', 'hour', 'a', 'day', 'it', 'still', 'looks', 'and', 'plays', 'beautifully', '.', 'overall', 'one', 'of', 'the', 'best', 'purchases', 'i', 'could', 'have', 'made', '.']\n",
            "\n",
            "\u001b[4mBefore removing stopwords and punctuations:\u001b[0m ['kit', 'is', 'awesome', '.', 'i', 'play', 'in', 'my', 'garage', 'just', 'for', 'personal', 'enjoyment', 'not', 'for', 'performances', 'or', 'anything', '.', 'once', 'you', 'take', 'the', 'time', 'to', 'break', 'down', 'all', 'the', 'settings', ',', 'your', 'able', 'to', 'dial', 'in', 'pretty', 'much', 'any', 'kit', 'and', 'sound', '.', 'with', 'the', 'expansion', 'options', 'and', 'the', 'relatively', 'inexpensive', 'parts', 'expanding', 'is', 'easy', 'and', 'fun', '.', 'after', 'a', 'few', 'weeks', 'of', 'daily', 'use', 'for', 'at', 'least', 'an', 'hour', 'a', 'day', 'it', 'still', 'looks', 'and', 'plays', 'beautifully', '.', 'overall', 'one', 'of', 'the', 'best', 'purchases', 'i', 'could', 'have', 'made', '.']\n",
            "\n",
            "\u001b[4mAfter removing stopwords and punctuations:\u001b[0m ['kit', 'awesome', 'play', 'garage', 'personal', 'enjoyment', 'performances', 'anything', 'take', 'time', 'break', 'settings', 'able', 'dial', 'pretty', 'much', 'kit', 'sound', 'expansion', 'options', 'relatively', 'inexpensive', 'parts', 'expanding', 'easy', 'fun', 'weeks', 'daily', 'use', 'least', 'hour', 'day', 'still', 'looks', 'plays', 'beautifully', 'overall', 'one', 'best', 'purchases', 'could', 'made']\n",
            "\n",
            "\u001b[4mBefore removing blank space:\u001b[0m ['kit', 'awesome', 'play', 'garage', 'personal', 'enjoyment', 'performances', 'anything', 'take', 'time', 'break', 'settings', 'able', 'dial', 'pretty', 'much', 'kit', 'sound', 'expansion', 'options', 'relatively', 'inexpensive', 'parts', 'expanding', 'easy', 'fun', 'weeks', 'daily', 'use', 'least', 'hour', 'day', 'still', 'looks', 'plays', 'beautifully', 'overall', 'one', 'best', 'purchases', 'could', 'made']\n",
            "\n",
            "\u001b[4mAfter removing blank space:\u001b[0m ['kit', 'awesome', 'play', 'garage', 'personal', 'enjoyment', 'performances', 'anything', 'take', 'time', 'break', 'settings', 'able', 'dial', 'pretty', 'much', 'kit', 'sound', 'expansion', 'options', 'relatively', 'inexpensive', 'parts', 'expanding', 'easy', 'fun', 'weeks', 'daily', 'use', 'least', 'hour', 'day', 'still', 'looks', 'plays', 'beautifully', 'overall', 'one', 'best', 'purchases', 'could', 'made']\n",
            "\n",
            "************************************************\n",
            "\n",
            "\u001b[4mBefore lowercasing:\u001b[0m I own 3 of these Minis.  Quality on this one is the worst. Glue joints at the neck/body were messy. Had a couple of small dings (see photo of one under strings near bridge) but the important stuff is OK ( neck is straight and the bridge is sound). Fret edges are very smooth. Sounds great but not as good as the Ovangkol.\n",
            "\n",
            "\u001b[4mAfter lowercasing:\u001b[0m i own 3 of these minis.  quality on this one is the worst. glue joints at the neck/body were messy. had a couple of small dings (see photo of one under strings near bridge) but the important stuff is ok ( neck is straight and the bridge is sound). fret edges are very smooth. sounds great but not as good as the ovangkol.\n",
            "\n",
            "\u001b[4mBefore tokenization:\u001b[0m i own 3 of these minis.  quality on this one is the worst. glue joints at the neck/body were messy. had a couple of small dings (see photo of one under strings near bridge) but the important stuff is ok ( neck is straight and the bridge is sound). fret edges are very smooth. sounds great but not as good as the ovangkol.\n",
            "\n",
            "\u001b[4mAfter tokenization:\u001b[0m ['i', 'own', '3', 'of', 'these', 'minis', '.', 'quality', 'on', 'this', 'one', 'is', 'the', 'worst', '.', 'glue', 'joints', 'at', 'the', 'neck/body', 'were', 'messy', '.', 'had', 'a', 'couple', 'of', 'small', 'dings', '(', 'see', 'photo', 'of', 'one', 'under', 'strings', 'near', 'bridge', ')', 'but', 'the', 'important', 'stuff', 'is', 'ok', '(', 'neck', 'is', 'straight', 'and', 'the', 'bridge', 'is', 'sound', ')', '.', 'fret', 'edges', 'are', 'very', 'smooth', '.', 'sounds', 'great', 'but', 'not', 'as', 'good', 'as', 'the', 'ovangkol', '.']\n",
            "\n",
            "\u001b[4mBefore removing stopwords and punctuations:\u001b[0m ['i', 'own', '3', 'of', 'these', 'minis', '.', 'quality', 'on', 'this', 'one', 'is', 'the', 'worst', '.', 'glue', 'joints', 'at', 'the', 'neck/body', 'were', 'messy', '.', 'had', 'a', 'couple', 'of', 'small', 'dings', '(', 'see', 'photo', 'of', 'one', 'under', 'strings', 'near', 'bridge', ')', 'but', 'the', 'important', 'stuff', 'is', 'ok', '(', 'neck', 'is', 'straight', 'and', 'the', 'bridge', 'is', 'sound', ')', '.', 'fret', 'edges', 'are', 'very', 'smooth', '.', 'sounds', 'great', 'but', 'not', 'as', 'good', 'as', 'the', 'ovangkol', '.']\n",
            "\n",
            "\u001b[4mAfter removing stopwords and punctuations:\u001b[0m ['3', 'minis', 'quality', 'one', 'worst', 'glue', 'joints', 'neck/body', 'messy', 'couple', 'small', 'dings', 'see', 'photo', 'one', 'strings', 'near', 'bridge', 'important', 'stuff', 'ok', 'neck', 'straight', 'bridge', 'sound', 'fret', 'edges', 'smooth', 'sounds', 'great', 'good', 'ovangkol']\n",
            "\n",
            "\u001b[4mBefore removing blank space:\u001b[0m ['3', 'minis', 'quality', 'one', 'worst', 'glue', 'joints', 'neck/body', 'messy', 'couple', 'small', 'dings', 'see', 'photo', 'one', 'strings', 'near', 'bridge', 'important', 'stuff', 'ok', 'neck', 'straight', 'bridge', 'sound', 'fret', 'edges', 'smooth', 'sounds', 'great', 'good', 'ovangkol']\n",
            "\n",
            "\u001b[4mAfter removing blank space:\u001b[0m ['3', 'minis', 'quality', 'one', 'worst', 'glue', 'joints', 'neck/body', 'messy', 'couple', 'small', 'dings', 'see', 'photo', 'one', 'strings', 'near', 'bridge', 'important', 'stuff', 'ok', 'neck', 'straight', 'bridge', 'sound', 'fret', 'edges', 'smooth', 'sounds', 'great', 'good', 'ovangkol']\n",
            "\n",
            "************************************************\n",
            "\n",
            "\u001b[4mBefore lowercasing:\u001b[0m This tailpiece is very solid and the plating and finish couldn't be better. So great for the price. I don't expect plating will stand contact/rubbing for long, but that's the case with most gold plated hardware, so I suggest any gold hardware be handled delicately. I know gold plating costs so much more than its price, so it's a bargain.\n",
            "\n",
            "\u001b[4mAfter lowercasing:\u001b[0m this tailpiece is very solid and the plating and finish couldn't be better. so great for the price. i don't expect plating will stand contact/rubbing for long, but that's the case with most gold plated hardware, so i suggest any gold hardware be handled delicately. i know gold plating costs so much more than its price, so it's a bargain.\n",
            "\n",
            "\u001b[4mBefore tokenization:\u001b[0m this tailpiece is very solid and the plating and finish couldn't be better. so great for the price. i don't expect plating will stand contact/rubbing for long, but that's the case with most gold plated hardware, so i suggest any gold hardware be handled delicately. i know gold plating costs so much more than its price, so it's a bargain.\n",
            "\n",
            "\u001b[4mAfter tokenization:\u001b[0m ['this', 'tailpiece', 'is', 'very', 'solid', 'and', 'the', 'plating', 'and', 'finish', 'could', \"n't\", 'be', 'better', '.', 'so', 'great', 'for', 'the', 'price', '.', 'i', 'do', \"n't\", 'expect', 'plating', 'will', 'stand', 'contact/rubbing', 'for', 'long', ',', 'but', 'that', \"'s\", 'the', 'case', 'with', 'most', 'gold', 'plated', 'hardware', ',', 'so', 'i', 'suggest', 'any', 'gold', 'hardware', 'be', 'handled', 'delicately', '.', 'i', 'know', 'gold', 'plating', 'costs', 'so', 'much', 'more', 'than', 'its', 'price', ',', 'so', 'it', \"'s\", 'a', 'bargain', '.']\n",
            "\n",
            "\u001b[4mBefore removing stopwords and punctuations:\u001b[0m ['this', 'tailpiece', 'is', 'very', 'solid', 'and', 'the', 'plating', 'and', 'finish', 'could', \"n't\", 'be', 'better', '.', 'so', 'great', 'for', 'the', 'price', '.', 'i', 'do', \"n't\", 'expect', 'plating', 'will', 'stand', 'contact/rubbing', 'for', 'long', ',', 'but', 'that', \"'s\", 'the', 'case', 'with', 'most', 'gold', 'plated', 'hardware', ',', 'so', 'i', 'suggest', 'any', 'gold', 'hardware', 'be', 'handled', 'delicately', '.', 'i', 'know', 'gold', 'plating', 'costs', 'so', 'much', 'more', 'than', 'its', 'price', ',', 'so', 'it', \"'s\", 'a', 'bargain', '.']\n",
            "\n",
            "\u001b[4mAfter removing stopwords and punctuations:\u001b[0m ['tailpiece', 'solid', 'plating', 'finish', 'could', \"n't\", 'better', 'great', 'price', \"n't\", 'expect', 'plating', 'stand', 'contact/rubbing', 'long', \"'s\", 'case', 'gold', 'plated', 'hardware', 'suggest', 'gold', 'hardware', 'handled', 'delicately', 'know', 'gold', 'plating', 'costs', 'much', 'price', \"'s\", 'bargain']\n",
            "\n",
            "\u001b[4mBefore removing blank space:\u001b[0m ['tailpiece', 'solid', 'plating', 'finish', 'could', \"n't\", 'better', 'great', 'price', \"n't\", 'expect', 'plating', 'stand', 'contact/rubbing', 'long', \"'s\", 'case', 'gold', 'plated', 'hardware', 'suggest', 'gold', 'hardware', 'handled', 'delicately', 'know', 'gold', 'plating', 'costs', 'much', 'price', \"'s\", 'bargain']\n",
            "\n",
            "\u001b[4mAfter removing blank space:\u001b[0m ['tailpiece', 'solid', 'plating', 'finish', 'could', \"n't\", 'better', 'great', 'price', \"n't\", 'expect', 'plating', 'stand', 'contact/rubbing', 'long', \"'s\", 'case', 'gold', 'plated', 'hardware', 'suggest', 'gold', 'hardware', 'handled', 'delicately', 'know', 'gold', 'plating', 'costs', 'much', 'price', \"'s\", 'bargain']\n",
            "\n",
            "************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question2\n",
        "loc2 = \"/content/drive/MyDrive/IR_Dataset/\"\n",
        "def do_pp(text):\n",
        "  text = text.lower()\n",
        "  tokens = word_tokenize(text)\n",
        "  tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]\n",
        "  tokens = [word for word in tokens if word.strip()]\n",
        "  return tokens\n",
        "\n",
        "class InvertedIndex:\n",
        "    def __init__(self):\n",
        "        self.index = {}\n",
        "\n",
        "    def index_document(self, document_name, text):\n",
        "        # Preprocess the text\n",
        "        words = preprocess_file(loc2+filename, 0)\n",
        "\n",
        "        # Update the index\n",
        "        for word in words:\n",
        "            if word not in self.index:\n",
        "                self.index[word] = {document_name}\n",
        "            else:\n",
        "                self.index[word].add(document_name)\n",
        "\n",
        "    def save(self, filename):\n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump(self.index, f)\n",
        "\n",
        "    def load(self, filename):\n",
        "        with open(filename, 'rb') as f:\n",
        "            self.index = pickle.load(f)\n",
        "\n",
        "    def query(self, query):\n",
        "        # Split the query into words and operators\n",
        "        elements = re.split(r' (AND|OR|NOT) ', query)\n",
        "\n",
        "        # Start with the set of all documents\n",
        "        result = set(self.index.keys())\n",
        "\n",
        "        # Apply each word/operator to the result\n",
        "        i = 0\n",
        "        while i < len(elements):\n",
        "            if elements[i] == 'AND':\n",
        "                if i + 1 < len(elements) and elements[i + 1] == 'NOT':\n",
        "                    temp = set(self.index.keys()) - self.index.get(elements[i + 2], set())\n",
        "                    result = result & temp\n",
        "                    i += 3\n",
        "                else:\n",
        "                    result = result & self.index.get(elements[i + 1], set())\n",
        "                    i += 2\n",
        "            elif elements[i] == 'OR':\n",
        "                if i + 1 < len(elements) and elements[i + 1] == 'NOT':\n",
        "                    temp = set(self.index.keys()) - self.index.get(elements[i + 2], set())\n",
        "                    result = result | temp\n",
        "                    i += 3\n",
        "                else:\n",
        "                    result = result | self.index.get(elements[i + 1], set())\n",
        "                    i += 2\n",
        "            elif elements[i] == 'NOT':\n",
        "                result = result - self.index.get(elements[i + 1], set())\n",
        "                i += 2\n",
        "            else:\n",
        "                if elements[i] in self.index:\n",
        "                    result = set(self.index[elements[i]])\n",
        "                else:\n",
        "                    result = set()\n",
        "                i += 1\n",
        "\n",
        "        return result\n"
      ],
      "metadata": {
        "id": "Y3w-v8xmXM1r"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def execute_queries(index, queries):\n",
        "    for i, (sequence, operations) in enumerate(queries, 1):\n",
        "        sequence = re.sub(r'\\W+', ' ', sequence.lower())\n",
        "        words = sequence.split()\n",
        "\n",
        "        # Construct the query\n",
        "        query = []\n",
        "        for word, operation in zip(words, operations.split(', ')):\n",
        "            query.append(word)\n",
        "            query.append(operation)\n",
        "        query.append(words[-1])\n",
        "\n",
        "        # Execute the query\n",
        "        result = index.query(' '.join(query))\n",
        "\n",
        "        # Print the results\n",
        "        print(f'Query {i}: {\" \".join(query)}')\n",
        "        print(f'Number of documents retrieved for query {i}: {len(result)}')\n",
        "        print(f'Names of the documents retrieved for query {i}: {\", \".join(result)}')"
      ],
      "metadata": {
        "id": "HMxvOMFj3r2g"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory containing the text files\n",
        "directory = loc2\n",
        "\n",
        "index = InvertedIndex()\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith('.txt'):\n",
        "        with open(os.path.join(directory, filename), 'r') as f:\n",
        "            text = f.read()\n",
        "            index.index_document(filename, text)\n",
        "\n",
        "# Save the index to a file\n",
        "index.save('index.pkl')\n",
        "\n",
        "# Load the index from the file\n",
        "index.load('index.pkl')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6-hldHI3sBx",
        "outputId": "5749d11d-039e-4f7b-e106-20e0eb2765d7"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-128-9cac72885ed1>:12: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"html.parser\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the number of queries from the user\n",
        "N = int(input())\n",
        "\n",
        "# Read the queries from the user\n",
        "queries = []\n",
        "for _ in range(N):\n",
        "    sequence = input()\n",
        "    operations = input()\n",
        "    queries.append((sequence, operations))\n",
        "\n",
        "# Execute the queries\n",
        "execute_queries(index, queries)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "7Wcm9IgH1scp",
        "outputId": "70736136-9a91-4dc4-a403-e3f8c3f727be"
      },
      "execution_count": 170,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "guitar\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-170-e46edd36d875>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0moperations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mqueries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question3\n",
        "class PositionalIndex:\n",
        "    def __init__(self):\n",
        "        self.index = {}\n",
        "\n",
        "    def index_document(self, document_name, text):\n",
        "        # Preprocess the text\n",
        "        words = do_pp(text)\n",
        "\n",
        "        # Update the index\n",
        "        for position, word in enumerate(words):\n",
        "            if word not in self.index:\n",
        "                self.index[word] = {document_name: [position]}\n",
        "            else:\n",
        "                if document_name in self.index[word]:\n",
        "                    self.index[word][document_name].append(position)\n",
        "                else:\n",
        "                    self.index[word][document_name] = [position]\n",
        "\n",
        "    def save(self, filename):\n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump(self.index, f)\n",
        "\n",
        "    def load(self, filename):\n",
        "        with open(filename, 'rb') as f:\n",
        "            self.index = pickle.load(f)\n",
        "\n",
        "    def query(self, query):\n",
        "        # Split the query into words\n",
        "        words = do_pp(query)\n",
        "        print(words)\n",
        "\n",
        "        # Start with the set of all positions of the first word\n",
        "        result = self.index.get(words[0], {})\n",
        "\n",
        "        # Apply each word to the result\n",
        "        for i in range(1, len(words)):\n",
        "            word_positions = self.index.get(words[i], {})\n",
        "            for doc in list(result.keys()):\n",
        "                positions = [pos - i for pos in word_positions.get(doc, [])]\n",
        "                result[doc] = [pos for pos in result[doc] if pos in positions]\n",
        "\n",
        "        # Filter out the documents where the query does not exist\n",
        "        result = {doc: positions for doc, positions in result.items() if positions}\n",
        "\n",
        "        return result\n",
        "\n",
        "# Directory containing the text files\n",
        "directory = loc2\n",
        "\n",
        "# Create a positional index of the dataset\n",
        "index = PositionalIndex()\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith('.txt'):\n",
        "        with open(os.path.join(directory, filename), 'r') as f:\n",
        "            text = f.read()\n",
        "            index.index_document(filename, text)\n",
        "\n",
        "# Save the index to a file\n",
        "index.save('index2.pkl')\n",
        "\n",
        "# Load the index from the file\n",
        "index.load('index2.pkl')\n",
        "print(index.index[\"coffee\"])\n",
        "print(len(index.index[\"coffee\"]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iwiw4na8U9Bm",
        "outputId": "decc8f46-b05a-4204-e7f3-c4570567bc14"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'pp_file157.txt': [30], 'pp_file886.txt': [17]}\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the number of queries from the user\n",
        "N = int(input())\n",
        "\n",
        "# Read the queries from the user and execute them\n",
        "for i in range(N):\n",
        "    query = input()\n",
        "    result = index.query(query)\n",
        "    print(f'Number of documents retrieved for query {i+1} using positional index: {len(result)}')\n",
        "    print(f'Names of documents retrieved for query {i+1} using positional index: {\", \".join(result.keys())}')\n",
        "#everything acoustic bass ukuleles know smaller model available ukes violins"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_bhfEcl8MzI",
        "outputId": "d6e3856f-f63f-4e2b-efcb-07af9550de34"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "everything acoustic bass ukuleles know smaller model available ukes violins\n",
            "['everything', 'acoustic', 'bass', 'ukuleles', 'know', 'smaller', 'model', 'available', 'ukes', 'violins']\n",
            "Number of documents retrieved for query 1 using positional index: 1\n",
            "Names of documents retrieved for query 1 using positional index: pp_file3.txt\n"
          ]
        }
      ]
    }
  ]
}